import torch

from agents.agent import Agent

SAVE_MODEL_LABEL = "model"
SAVE_OPTIMIZER_LABEL = "optimizer"
SAVE_FILE_EXTENSION = ".pt"


def device_setup():
    cuda_available = torch.cuda.is_available()
    print(f"CUDA available: {cuda_available}")
    if cuda_available:
        print(f"CUDA version: {torch.version.cuda}")
        print(f"CUDA device count: {torch.cuda.device_count()}")
        cuda_id = torch.cuda.current_device()
        print(f"CUDA current device id: {cuda_id}")
        print(f"CUDA device name: {torch.cuda.get_device_name(cuda_id)}")
        torch.set_default_tensor_type("torch.cuda.FloatTensor")
    return torch.device("cuda" if torch.cuda.is_available() else "cpu")


def compress_model_str(model: torch.nn.Sequential):
    compressed_str = ""
    for layer in model.children():
        layer_str = str(layer)
        separated = layer_str[:-1].split("(")  # [:-1] Leaves the end parenthesis out
        if len(separated) > 1:
            layer_type, layer_info = separated
            layer_compressed_str = layer_type[0] + "("  # First letter of the layer type, and open parenthesis
            layer_hyper_params = layer_info.split(",")
            hyper_params_iter = iter(layer_hyper_params)
            layer_compressed_str += next(hyper_params_iter).split("=")[1]  # Get the second entry, should be the value
            for hyper_param in hyper_params_iter:
                _, val = hyper_param.split("=")
                layer_compressed_str += "," + val
            layer_compressed_str += ")"
        else:
            layer_compressed_str = separated[0]
        compressed_str += layer_compressed_str
    return compressed_str


def compress_optimizer_str(optimizer: torch.optim.Optimizer):
    split_info = str(optimizer).split("\n")
    info_iter = iter(split_info)
    optimizer_name = next(info_iter).split(" ")[0]  # <OptimizerName> (
    next(info_iter)  # Parameter Group 0
    compressed = optimizer_name + "("
    # Do first iteration manually, to prevent leading comma
    key, val = next(info_iter).split(": ")
    compressed += val
    for param in info_iter:
        if param == ")":
            break
        key, val = param.split(": ")
        compressed += "," + val
    compressed += ")"
    return compressed


class AgentPyTorch(Agent):
    def __init__(self):
        self.agent_name = None

    def init_vars(self):
        super().init_vars()

    def update(self):
        super().update()

    def activate(self, game):
        super().activate(game)

    def save_model(self, model: torch.nn.Sequential, optimizer: torch.optim.Optimizer, filename=None):
        """
        Save a model and its optimizer to a file
        :param model: The model to save
        :param optimizer: The optimizer of the model
        :param filename: The name of the file, set to None to generate a name based on the model parameters
        :return: None
        """
        if filename is None:
            filename = compress_model_str(model) + compress_optimizer_str(optimizer)
        to_save = {SAVE_MODEL_LABEL: model.state_dict(), SAVE_OPTIMIZER_LABEL: optimizer.state_dict()}
        filename = f"({self.agent_name}){filename}{SAVE_FILE_EXTENSION}"
        torch.save(to_save, filename)

    def load_model(self, model: torch.nn.Sequential, optimizer: torch.optim.Optimizer, filename=None):
        """
        Load a model and its optimizer from a file generated by the save_model function
        :param model: The model to load into
        :param optimizer: The optimizer of the model to load into
        :param filename: The name of the file to load from
        :return: None
        """
        if filename is None:
            filename = compress_model_str(model) + compress_optimizer_str(optimizer)
        filename = f"({self.agent_name}){filename}{SAVE_FILE_EXTENSION}"
        try:
            loaded = torch.load(filename)
            model.load_state_dict(loaded[SAVE_MODEL_LABEL])
            optimizer.load_state_dict(loaded[SAVE_OPTIMIZER_LABEL])
        except FileNotFoundError:
            print(f"Could not load model, file not found: \n{filename}")
